{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Dans l'autre notebook)\n",
    "import pandas as pd\n",
    "df_for_ml = pd.read_csv(\"./df_for_ml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_ml[\"emotion_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364e6a2",
   "metadata": {},
   "source": [
    "- Séparation en train - Cross Validation - Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02980e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in df_for_ml.columns if c.startswith(\"f_\")]\n",
    "X = df_for_ml[feature_cols].to_numpy(dtype=np.float32)\n",
    "y = df_for_ml[\"emotion_label\"].to_numpy()\n",
    "\n",
    "# 75% train, 15% val, 10% test\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.15, random_state=42, stratify=y_tmp\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)                      # fit sur TRAIN uniquement\n",
    "y_train_int = le.transform(y_train)\n",
    "y_val_int   = le.transform(y_val)\n",
    "y_test_int  = le.transform(y_test)\n",
    "\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a712bc",
   "metadata": {},
   "source": [
    "- Centrer réduire les données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec04ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# créer l'objet de centrage réduction \n",
    "scaler = StandardScaler()                \n",
    "\n",
    "# Apprendre les paramètres (moyenne/écart-type) sur le TRAIN uniquement\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "\n",
    "# Réutiliser le MÊME scaler pour centrer/réduire VAL et TEST (pas de fit ici)\n",
    "X_val   = scaler.transform(X_val).astype(np.float32)\n",
    "X_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece79e4",
   "metadata": {},
   "source": [
    "# Conception des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ee7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamètres\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_mse_onehot(num_classes: int):\n",
    "    @tf.function\n",
    "    def mse_onehot(y_true, y_pred):\n",
    "        # y_true peut être (batch,) ou (batch,1) -> on squeeze et on cast\n",
    "        y_true = tf.cast(tf.squeeze(y_true), tf.int32)\n",
    "        y_true_oh = tf.one_hot(y_true, depth=num_classes)\n",
    "        return tf.reduce_mean(tf.math.squared_difference(y_true_oh, y_pred))\n",
    "    return mse_onehot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568b6eb",
   "metadata": {},
   "source": [
    "- Hyperparamètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f70b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3  # ajuste si besoin\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca00e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "def build_model_A(input_dim: int, num_classes: int,\n",
    "                  dropout: float = 0.0, lambda_reg: float = 0.0) -> keras.Model:\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    reg = keras.regularizers.l2(lambda_reg)\n",
    "\n",
    "    x = layers.Dense(32, activation=\"relu\",\n",
    "                     kernel_regularizer=reg,\n",
    "                     kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    probs = layers.Dense(num_classes, activation=\"softmax\", name=\"softmax_layer\")(x)\n",
    "    model = keras.Model(inputs, probs, name=\"Model_A\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_B(input_dim: int, num_classes: int,\n",
    "                  dropout: float = 0.0, lambda_reg: float = 0.0) -> keras.Model:\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    reg = keras.regularizers.l2(lambda_reg)\n",
    "\n",
    "    x = layers.Dense(64, activation=\"relu\",\n",
    "                     kernel_regularizer=reg,\n",
    "                     kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\",\n",
    "                     kernel_regularizer=reg,\n",
    "                     kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    probs = layers.Dense(num_classes, activation=\"softmax\", name=\"softmax_layer\")(x)\n",
    "    model = keras.Model(inputs, probs, name=\"Model_B\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_C(input_dim: int, num_classes: int,\n",
    "                  dropout: float = 0.0, lambda_reg: float = 0.0) -> keras.Model:\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    reg = keras.regularizers.l2(lambda_reg)\n",
    "    he_init = keras.initializers.HeNormal()\n",
    "\n",
    "    x = layers.Dense(100, activation=\"relu\",\n",
    "                     kernel_regularizer=reg,\n",
    "                     kernel_initializer=he_init)(inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\",\n",
    "                     kernel_regularizer=reg,\n",
    "                     kernel_initializer=he_init)(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    probs = layers.Dense(num_classes, activation=\"softmax\", name=\"softmax_layer\")(x)\n",
    "    model = keras.Model(inputs, probs, name=\"Model_C\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_D(input_dim: int, num_classes: int,\n",
    "                  dropout: float = 0.0, lambda_reg: float = 0.0) -> keras.Model:\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    reg = keras.regularizers.l2(lambda_reg)\n",
    "\n",
    "    x = layers.Dense(256, activation=\"relu\",\n",
    "                     kernel_regularizer=reg,\n",
    "                     kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\",\n",
    "                     kernel_regularizer=reg,\n",
    "                     kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\",\n",
    "                     kernel_regularizer=reg,\n",
    "                     kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    probs = layers.Dense(num_classes, activation=\"softmax\", name=\"softmax_layer\")(x)\n",
    "    model = keras.Model(inputs, probs, name=\"Model_D\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, min_lr=1e-5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e12fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Instanciation ====\n",
    "model_A = build_model_A(input_dim, num_classes)\n",
    "model_B = build_model_B(input_dim, num_classes)\n",
    "model_C = build_model_C(input_dim, num_classes)\n",
    "model_D = build_model_D(input_dim, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ==== Entraînement ====\n",
    "\n",
    "\n",
    "histories = {}\n",
    "print(\"\\n=== Entraînement model_A ===\")\n",
    "histories[\"A\"] = model_A.fit(\n",
    "    X_train, y_train_int, validation_data=(X_val, y_val_int),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE,  verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=== Entraînement model_B ===\")\n",
    "histories[\"B\"] = model_B.fit(\n",
    "    X_train, y_train_int, validation_data=(X_val, y_val_int),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n=== Entraînement model_C ===\")\n",
    "histories[\"C\"] = model_C.fit(\n",
    "    X_train, y_train_int, validation_data=(X_val, y_val_int),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=== Entraînement model_D ===\")\n",
    "histories[\"D\"] = model_D.fit(\n",
    "    X_train, y_train_int, validation_data=(X_val, y_val_int),\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12560f90",
   "metadata": {},
   "source": [
    "- Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure integer labels if yours are strings\n",
    "def ensure_int(y):\n",
    "    y = np.asarray(y)\n",
    "    if y.dtype.kind in (\"U\",\"S\",\"O\"):\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    return y\n",
    "\n",
    "y_train_int = ensure_int(y_train)\n",
    "y_val_int   = ensure_int(y_val)\n",
    "\n",
    "models = [\n",
    "    (\"A (peu profond)\",  model_A),\n",
    "    (\"B\",                model_B),\n",
    "    (\"C\",                model_C),\n",
    "    (\"D (plus profond)\", model_D),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, model in models:\n",
    "    # Predict softmax probabilities, then class ids\n",
    "    p_tr = model.predict(X_train, verbose=0)\n",
    "    p_va = model.predict(X_val,   verbose=0)\n",
    "    yhat_tr = p_tr.argmax(axis=1)\n",
    "    yhat_va = p_va.argmax(axis=1)\n",
    "\n",
    "    # Accuracy (train/val) and error\n",
    "    acc_train = accuracy_score(y_train_int, yhat_tr)\n",
    "    acc_val   = accuracy_score(y_val_int,   yhat_va)\n",
    "    err_train = 1.0 - acc_train\n",
    "    err_val   = 1.0 - acc_valp_tr = model.predict(X_train, verbose=0)\n",
    "    p_va = model.predict(X_val,   verbose=0)\n",
    "    yhat_tr = p_tr.argmax(axis=1)\n",
    "    yhat_va = p_va.argmax(axis=1)\n",
    "\n",
    "    # Accuracy (train/val) and error\n",
    "    acc_train = accuracy_score(y_train_int, yhat_tr)\n",
    "    acc_val   = accuracy_score(y_val_int,   yhat_va)\n",
    "    err_train = 1.0 - acc_train\n",
    "    err_val   = 1.0 - acc_val\n",
    "\n",
    "    # Validation precision/recall/F1 (weighted)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_val_int, yhat_va, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"acc_train\": acc_train,\n",
    "        \"err_train\": err_train,\n",
    "        \"acc_val\":   acc_val,\n",
    "        \"err_val\":   err_val,\n",
    "       \n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(results).sort_values(\"acc_val\", ascending=False).reset_index(drop=True)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b7749",
   "metadata": {},
   "source": [
    "#  Ajustement du modèle C \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_C_bis = build_model_C(input_dim, num_classes, dropout=0.0, lambda_reg=0.01)\n",
    "\n",
    "model_C_bis.compile(\n",
    "        optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "histories = {}\n",
    "print(\"\\n=== Entraînement model_C_bis ===\")\n",
    "histories[\"model_C_bis\"] = model_C_bis.fit(\n",
    "    X_train, y_train_int, validation_data=(X_val, y_val_int),\n",
    "    callbacks=callbacks, \n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE,  verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tr = model_C_bis.predict(X_train, verbose=0)\n",
    "p_va = model_C_bis.predict(X_val,   verbose=0)\n",
    "yhat_tr = p_tr.argmax(axis=1)\n",
    "yhat_va = p_va.argmax(axis=1)\n",
    "\n",
    "# Accuracy (train/val) and error\n",
    "acc_train = accuracy_score(y_train_int, yhat_tr)\n",
    "acc_val   = accuracy_score(y_val_int,   yhat_va)\n",
    "err_train = 1.0 - acc_train\n",
    "err_val   = 1.0 - acc_val\n",
    "\n",
    "\n",
    "print(\"Accuracy (train/val): %.2f / %.2f\" % (acc_train, acc_val))\n",
    "print(\"Error    (train/val): %.2f / %.2f\" % (err_train, err_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pour la reproductibilité\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# --- Grille d’hyperparamètres ---\n",
    "dropouts = [0.0, 0.2, 0.4, 0.6]\n",
    "lambdas  = [0.0, 1e-4, 1e-3, 1e-2, 3e-2]\n",
    "\n",
    "results = []\n",
    "histories = {}\n",
    "\n",
    "for d in dropouts:\n",
    "    for lam in lambdas:\n",
    "        print(f\"\\n=== Test dropout={d:.2f} | lambda_reg={lam:.4f} ===\")\n",
    "\n",
    "        # 1) (Re)construire un modèle neuf à chaque essai\n",
    "        model = build_model_C(input_dim, num_classes, dropout=d, lambda_reg=lam)\n",
    "        # Si build_model_C ne compile pas déjà :\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        # 2) Entraîner\n",
    "        h = model.fit(\n",
    "            X_train, y_train_int,\n",
    "            validation_data=(X_val, y_val_int),\n",
    "            epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "            callbacks=callbacks, verbose=0\n",
    "        )\n",
    "        histories[(d, lam)] = h.history\n",
    "\n",
    "        # 3) Évaluer (erreurs)\n",
    "        p_tr = model.predict(X_train, verbose=0)\n",
    "        p_va = model.predict(X_val,   verbose=0)\n",
    "        yhat_tr = p_tr.argmax(1)\n",
    "        yhat_va = p_va.argmax(1)\n",
    "\n",
    "        acc_train = accuracy_score(y_train_int, yhat_tr)\n",
    "        acc_val   = accuracy_score(y_val_int,   yhat_va)\n",
    "\n",
    "        results.append({\n",
    "            \"dropout\": d,\n",
    "            \"lambda_reg\": lam,\n",
    "            \"acc_train\": acc_train,\n",
    "            \"err_train\": 1.0 - acc_train,\n",
    "            \"acc_val\":   acc_val,\n",
    "            \"err_val\":   1.0 - acc_val,\n",
    "            \"best_val_loss\": float(np.min(h.history[\"val_loss\"]))\n",
    "        })\n",
    "\n",
    "# --- Tableau récapitulatif ---\n",
    "res_df = pd.DataFrame(results).sort_values(\"err_val\").reset_index(drop=True)\n",
    "print(\"\\nTop 10 (par plus faible erreur validation):\")\n",
    "print(res_df.head(10))\n",
    "\n",
    "# === 4) VISU : Heatmaps des erreurs ===\n",
    "# (a) Heatmap erreur validation\n",
    "pivot_val = res_df.pivot(index=\"dropout\", columns=\"lambda_reg\", values=\"err_val\").sort_index(ascending=True)\n",
    "plt.figure(figsize=(7,4))\n",
    "im = plt.imshow(pivot_val.values, aspect=\"auto\", origin=\"lower\")\n",
    "plt.xticks(ticks=range(len(pivot_val.columns)), labels=[f\"{c:.0e}\" if c>0 else \"0\" for c in pivot_val.columns])\n",
    "plt.yticks(ticks=range(len(pivot_val.index)),   labels=[f\"{r:.1f}\" for r in pivot_val.index])\n",
    "plt.colorbar(im, label=\"Validation error (1-acc)\")\n",
    "plt.xlabel(\"lambda_reg\"); plt.ylabel(\"dropout\")\n",
    "plt.title(\"Erreur validation selon (dropout, lambda)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# (b) Heatmap erreur train\n",
    "pivot_tr = res_df.pivot(index=\"dropout\", columns=\"lambda_reg\", values=\"err_train\").sort_index(ascending=True)\n",
    "plt.figure(figsize=(7,4))\n",
    "im = plt.imshow(pivot_tr.values, aspect=\"auto\", origin=\"lower\")\n",
    "plt.xticks(ticks=range(len(pivot_tr.columns)), labels=[f\"{c:.0e}\" if c>0 else \"0\" for c in pivot_tr.columns])\n",
    "plt.yticks(ticks=range(len(pivot_tr.index)),   labels=[f\"{r:.1f}\" for r in pivot_tr.index])\n",
    "plt.colorbar(im, label=\"Train error (1-acc)\")\n",
    "plt.xlabel(\"lambda_reg\"); plt.ylabel(\"dropout\")\n",
    "plt.title(\"Erreur train selon (dropout, lambda)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# === 5) VISU : Courbes erreur vs lambda pour chaque dropout ===\n",
    "plt.figure(figsize=(7,4))\n",
    "for d in dropouts:\n",
    "    sub = res_df[res_df[\"dropout\"]==d].sort_values(\"lambda_reg\")\n",
    "    plt.plot(sub[\"lambda_reg\"], sub[\"err_val\"], marker=\"o\", label=f\"dropout={d}\")\n",
    "plt.xscale(\"log\"); plt.xlabel(\"lambda_reg (log)\")\n",
    "plt.ylabel(\"Validation error (1-acc)\")\n",
    "plt.title(\"Erreur validation vs lambda (par dropout)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "for d in dropouts:\n",
    "    sub = res_df[res_df[\"dropout\"]==d].sort_values(\"lambda_reg\")\n",
    "    plt.plot(sub[\"lambda_reg\"], sub[\"err_train\"], marker=\"o\", label=f\"dropout={d}\")\n",
    "plt.xscale(\"log\"); plt.xlabel(\"lambda_reg (log)\")\n",
    "plt.ylabel(\"Train error (1-acc)\")\n",
    "plt.title(\"Erreur train vs lambda (par dropout)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# === 6) (Optionnel) Learning curves pour le meilleur couple ===\n",
    "best = res_df.iloc[0]\n",
    "best_hist = histories[(best[\"dropout\"], best[\"lambda_reg\"])]\n",
    "plt.figure()\n",
    "plt.plot(best_hist[\"loss\"], label=\"train\")\n",
    "plt.plot(best_hist[\"val_loss\"], label=\"val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\n",
    "    f\"Learning curves best: dropout={best['dropout']}, lambda={best['lambda_reg']:.0e}\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
